{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"INPUT_DIR = \"../input/blood-cells/dataset2-master/dataset2-master/images/\"\ntrain_dir = INPUT_DIR + \"TRAIN/\"\ntest_dir = INPUT_DIR + \"TEST/\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-20T13:11:15.861218Z","iopub.execute_input":"2022-02-20T13:11:15.862111Z","iopub.status.idle":"2022-02-20T13:11:15.887128Z","shell.execute_reply.started":"2022-02-20T13:11:15.861997Z","shell.execute_reply":"2022-02-20T13:11:15.886208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install --upgrade git+https://github.com/broadinstitute/keras-resnet\nimport keras\nimport keras_resnet\n\nprint(\"imported\")","metadata":{"execution":{"iopub.status.busy":"2022-02-20T13:13:44.751112Z","iopub.execute_input":"2022-02-20T13:13:44.751435Z","iopub.status.idle":"2022-02-20T13:14:04.758112Z","shell.execute_reply.started":"2022-02-20T13:13:44.751406Z","shell.execute_reply":"2022-02-20T13:14:04.756929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# resnets_utils.py\n\nimport os\nimport numpy as np\nimport tensorflow as tf\nimport h5py\nimport math\n\ndef load_dataset():\n    train_dataset = h5py.File('datasets/train_signs.h5', \"r\")\n    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n\n    test_dataset = h5py.File('datasets/test_signs.h5', \"r\")\n    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n\n    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n    \n    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n    \n    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes\n\n\ndef random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n    \"\"\"\n    Creates a list of random minibatches from (X, Y)\n    \n    Arguments:\n    X -- input data, of shape (input size, number of examples) (m, Hi, Wi, Ci)\n    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples) (m, n_y)\n    mini_batch_size - size of the mini-batches, integer\n    seed -- this is only for the purpose of grading, so that you're \"random minibatches are the same as ours.\n    \n    Returns:\n    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n    \"\"\"\n    \n    m = X.shape[0]                  # number of training examples\n    mini_batches = []\n    np.random.seed(seed)\n    \n    # Step 1: Shuffle (X, Y)\n    permutation = list(np.random.permutation(m))\n    shuffled_X = X[permutation,:,:,:]\n    shuffled_Y = Y[permutation,:]\n\n    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n    for k in range(0, num_complete_minibatches):\n        mini_batch_X = shuffled_X[k * mini_batch_size : k * mini_batch_size + mini_batch_size,:,:,:]\n        mini_batch_Y = shuffled_Y[k * mini_batch_size : k * mini_batch_size + mini_batch_size,:]\n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n    \n    # Handling the end case (last mini-batch < mini_batch_size)\n    if m % mini_batch_size != 0:\n        mini_batch_X = shuffled_X[num_complete_minibatches * mini_batch_size : m,:,:,:]\n        mini_batch_Y = shuffled_Y[num_complete_minibatches * mini_batch_size : m,:]\n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n    \n    return mini_batches\n\n\ndef convert_to_one_hot(Y, C):\n    Y = np.eye(C)[Y.reshape(-1)].T\n    return Y\n\n\ndef forward_propagation_for_predict(X, parameters):\n    \"\"\"\n    Implements the forward propagation for the model: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SOFTMAX\n    \n    Arguments:\n    X -- input dataset placeholder, of shape (input size, number of examples)\n    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\"\n                  the shapes are given in initialize_parameters\n    Returns:\n    Z3 -- the output of the last LINEAR unit\n    \"\"\"\n    \n    # Retrieve the parameters from the dictionary \"parameters\" \n    W1 = parameters['W1']\n    b1 = parameters['b1']\n    W2 = parameters['W2']\n    b2 = parameters['b2']\n    W3 = parameters['W3']\n    b3 = parameters['b3'] \n                                                           # Numpy Equivalents:\n    Z1 = tf.add(tf.matmul(W1, X), b1)                      # Z1 = np.dot(W1, X) + b1\n    A1 = tf.nn.relu(Z1)                                    # A1 = relu(Z1)\n    Z2 = tf.add(tf.matmul(W2, A1), b2)                     # Z2 = np.dot(W2, a1) + b2\n    A2 = tf.nn.relu(Z2)                                    # A2 = relu(Z2)\n    Z3 = tf.add(tf.matmul(W3, A2), b3)                     # Z3 = np.dot(W3,Z2) + b3\n    \n    return Z3\n\ndef predict(X, parameters):\n    \n    W1 = tf.convert_to_tensor(parameters[\"W1\"])\n    b1 = tf.convert_to_tensor(parameters[\"b1\"])\n    W2 = tf.convert_to_tensor(parameters[\"W2\"])\n    b2 = tf.convert_to_tensor(parameters[\"b2\"])\n    W3 = tf.convert_to_tensor(parameters[\"W3\"])\n    b3 = tf.convert_to_tensor(parameters[\"b3\"])\n    \n    params = {\"W1\": W1,\n              \"b1\": b1,\n              \"W2\": W2,\n              \"b2\": b2,\n              \"W3\": W3,\n              \"b3\": b3}\n    \n    x = tf.placeholder(\"float\", [12288, 1])\n    \n    z3 = forward_propagation_for_predict(x, params)\n    p = tf.argmax(z3)\n    \n    sess = tf.Session()\n    prediction = sess.run(p, feed_dict = {x: X})\n        \n    return prediction\n\nprint(\"completed\")","metadata":{"execution":{"iopub.status.busy":"2022-02-20T13:14:15.46341Z","iopub.execute_input":"2022-02-20T13:14:15.46373Z","iopub.status.idle":"2022-02-20T13:14:15.488036Z","shell.execute_reply.started":"2022-02-20T13:14:15.463699Z","shell.execute_reply":"2022-02-20T13:14:15.486876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt\nimport os\nfrom urllib.request import urlopen,urlretrieve\nfrom PIL import Image\nfrom tqdm import tqdm_notebook\n%matplotlib inline\nfrom sklearn.utils import shuffle\nimport cv2\n# from resnets_utils import *\n\nfrom keras.models import load_model\nfrom sklearn.datasets import load_files   \nfrom keras.utils import np_utils\nfrom glob import glob\nfrom keras import applications\nfrom keras.preprocessing.image import ImageDataGenerator \nfrom keras import optimizers\nfrom keras.models import Sequential,Model,load_model\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D,GlobalAveragePooling2D\nfrom keras.callbacks import TensorBoard,ReduceLROnPlateau,ModelCheckpoint\n\n# -----------------------------------------------\nimport os\nimport gc\nimport numpy as np \nimport pandas as pd\nimport time\nimport cv2\n# import tensorflow as tf\nfrom tqdm import tqdm\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split, StratifiedShuffleSplit\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.applications.vgg16 import VGG16 as tmodel_vgg16\nfrom keras.utils.np_utils import to_categorical\nfrom tensorflow.keras.applications.resnet50 import ResNet50\n\nfrom keras.applications.xception import Xception as tmodel_xception\nfrom keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D,MaxPooling2D\nfrom keras.layers import Input,SeparableConv2D,Conv2D\nfrom keras.models import Sequential, Model \nfrom keras.losses import categorical_crossentropy as loss\nfrom tensorflow.keras.optimizers import SGD,Adam\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom sklearn.preprocessing import OneHotEncoder\nfrom keras.callbacks import ModelCheckpoint,LearningRateScheduler\nfrom keras.callbacks import EarlyStopping,CSVLogger,ReduceLROnPlateau,TensorBoard\n# from keras.layers.normalization import BatchNormalization\nfrom tensorflow.keras.layers import BatchNormalization\nfrom keras.layers import Concatenate\nfrom tensorflow.keras.utils import to_categorical\nfrom keras.layers import Add\nimport numpy as np\nimport keras\nimport pandas as pd\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv2D, BatchNormalization, Dropout, MaxPool2D, Input, Softmax, Activation, Flatten\nfrom keras.models import Model\nfrom keras import optimizers\nfrom keras.layers import SeparableConv2D, Concatenate\nfrom keras.layers import concatenate,AveragePooling2D\n# from keras.objectives import sparse_categorical_crossentropy as scc\nfrom tensorflow.keras.metrics import sparse_categorical_crossentropy as scc\nfrom keras.losses import categorical_crossentropy as cc\nimport os\nimport cv2\nimport scipy\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.utils.vis_utils import plot_model\nimport matplotlib.pyplot as plt\n# print(os.listdir(\"/content/gdrive/MyDrive/4-2\"))\nprint(\"Loaded\")","metadata":{"execution":{"iopub.status.busy":"2022-02-20T13:14:16.933058Z","iopub.execute_input":"2022-02-20T13:14:16.933531Z","iopub.status.idle":"2022-02-20T13:14:18.20798Z","shell.execute_reply.started":"2022-02-20T13:14:16.933499Z","shell.execute_reply":"2022-02-20T13:14:18.206867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"height = 128\nwidth = 128\nchannels = 3\n\nn_classes = 5\ninput_shape = (height, width, channels)\n\nepochs = 50\nbatch_size = 32","metadata":{"execution":{"iopub.status.busy":"2022-02-20T13:14:19.854155Z","iopub.execute_input":"2022-02-20T13:14:19.85471Z","iopub.status.idle":"2022-02-20T13:14:19.860966Z","shell.execute_reply.started":"2022-02-20T13:14:19.854659Z","shell.execute_reply":"2022-02-20T13:14:19.860082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def resize_img(img, shape):\n    return cv2.resize(img, (shape[1], shape[0]), interpolation=cv2.INTER_CUBIC)\n\n# Thanks to Paul Moonely for How to load data\ndef get_data(folder):\n    \"\"\"\n    Load the data and labels from the given folder.\n    \"\"\"\n    X = []\n    y = []\n    z = []\n    for wbc_type in os.listdir(folder):\n        if not wbc_type.startswith('.'):\n            if wbc_type in ['NEUTROPHIL']:\n                label = 1\n                label2 = 1\n            elif wbc_type in ['EOSINOPHIL']:\n                label = 2\n                label2 = 1\n            elif wbc_type in ['MONOCYTE']:\n                label = 3  \n                label2 = 0\n            elif wbc_type in ['LYMPHOCYTE']:\n                label = 4 \n                label2 = 0\n            else:\n                label = 5\n                label2 = 0\n            for image_filename in tqdm(os.listdir(folder + wbc_type)):\n                img_file = cv2.imread(folder + wbc_type + '/' + image_filename)\n                if img_file is not None:\n                  #input must have a static square shape (one of (128, 128), (160, 160), (192, 192), or (224, 224))\n#                     img_file = scipy.misc.imresize(arr=img_file, size=(128, 128, 3))\n                    img_file = resize_img(img_file, (height, width))\n                    img_arr = np.asarray(img_file)\n                    X.append(img_arr)\n                    y.append(label)\n                    z.append(label2)\n    X = np.asarray(X)\n    y = np.asarray(y)\n    z = np.asarray(z)\n    return X,y,z\nprint(\"Completed\")","metadata":{"execution":{"iopub.status.busy":"2022-02-20T13:14:21.364835Z","iopub.execute_input":"2022-02-20T13:14:21.365243Z","iopub.status.idle":"2022-02-20T13:14:21.381014Z","shell.execute_reply.started":"2022-02-20T13:14:21.365212Z","shell.execute_reply":"2022-02-20T13:14:21.379867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_trn, y_trn, z_trn = get_data(train_dir)\nx_test, y_test, z_test = get_data(test_dir)\nprint(\"Completed\")","metadata":{"execution":{"iopub.status.busy":"2022-02-20T13:14:22.629354Z","iopub.execute_input":"2022-02-20T13:14:22.630179Z","iopub.status.idle":"2022-02-20T13:15:28.935416Z","shell.execute_reply.started":"2022-02-20T13:14:22.630109Z","shell.execute_reply":"2022-02-20T13:15:28.93418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### normal preprocessing","metadata":{}},{"cell_type":"code","source":"print(x_trn.shape)\nprint(y_trn.shape)\nprint(x_test.shape)\nprint(y_test.shape)\nX_train_orig, Y_train_orig, X_test_orig, Y_test_orig = x_trn, y_trn, x_test, y_test\n\nX_train = X_train_orig/255.\nX_test = X_test_orig/255.\n\nY_train = to_categorical(Y_train_orig,5)\nY_test = to_categorical(Y_test_orig,5)\nz_trnHot = to_categorical(z_trn,2)\nz_testHot = to_categorical(z_test, num_classes = 2)\ndict_characters = {1:'NEUTROPHIL',2:'EOSINOPHIL',3:'MONOCYTE',4:'LYMPHOCYTE'}\ndict_characters2 = {0:'Mononuclear',1:'Polynuclear'}\nprint(dict_characters)\n# print(dict_characters2)\n\n# print(Y_train.shape)\n# print(Y_test.shape)\n\ndef do_shuffling(X, y):\n  index_array = np.arange(X.shape[0])\n  sample_index = np.random.choice(index_array, X.shape[0], replace=False)\n\n  X = X[sample_index, :, :, :]\n  y = y[sample_index, : ]\n\n  return X, y\n\nX_train, Y_train = do_shuffling(X_train, Y_train)\n\nprint(X_train.shape)\nprint(Y_train.shape)\nprint(X_test.shape)\nprint(Y_test.shape)\n\nprint('Completed')","metadata":{"execution":{"iopub.status.busy":"2022-02-20T12:20:27.110331Z","iopub.execute_input":"2022-02-20T12:20:27.110652Z","iopub.status.idle":"2022-02-20T12:20:30.963463Z","shell.execute_reply.started":"2022-02-20T12:20:27.110618Z","shell.execute_reply":"2022-02-20T12:20:30.962345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### batch preprocessing","metadata":{}},{"cell_type":"code","source":"X_train_orig, Y_train_orig, X_test_orig, Y_test_orig = x_trn, y_trn, x_test, y_test\ndict_characters = {1:'NEUTROPHIL',2:'EOSINOPHIL',3:'MONOCYTE',4:'LYMPHOCYTE'}\n\nn_classes = 5\ny_train = to_categorical(Y_train_orig, n_classes)\ny_test = to_categorical(Y_test_orig, n_classes)\n\nX_train = X_train_orig\nX_test = X_test_orig\n\n\ndef do_shuffling(X, y):\n  index_array = np.arange(X.shape[0])\n  sample_index = np.random.choice(index_array, X.shape[0], replace=False)\n\n  X = X[sample_index, :, :, :]\n  y = y[sample_index, : ]\n\n  return X, y\n\nX_train, y_train = do_shuffling(X_train, y_train)\n\n\n\n#using stratified shuffle split to preserve the percentage of samples in each of the 100 classes\nsss = StratifiedShuffleSplit(n_splits=2, test_size=0.2, random_state=123)\n\nfor train_index, val_index in sss.split(X_train, y_train):\n    X_train_data, X_val_data = X_train[train_index], X_train[val_index]\n    y_train_data, y_val_data = y_train[train_index], y_train[val_index]\n\nprint(\"Number of training samples: \", X_train_data.shape)\nprint(\"Number of validation samples: \", X_val_data.shape)\n\nX_test_data = X_test\ny_test_data = y_test","metadata":{"execution":{"iopub.status.busy":"2022-02-20T13:16:43.984947Z","iopub.execute_input":"2022-02-20T13:16:43.985444Z","iopub.status.idle":"2022-02-20T13:16:44.661003Z","shell.execute_reply.started":"2022-02-20T13:16:43.985409Z","shell.execute_reply":"2022-02-20T13:16:44.659754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#resizing the images as per EfficientNetB0 to size (224, 224)\nheight = 128\nwidth = 128\nchannels = 3\n\nn_classes = 5\ninput_shape = (height, width, channels)\n\nepochs = 50\nbatch_size = 32","metadata":{"execution":{"iopub.status.busy":"2022-02-20T13:16:47.229633Z","iopub.execute_input":"2022-02-20T13:16:47.230275Z","iopub.status.idle":"2022-02-20T13:16:47.23696Z","shell.execute_reply.started":"2022-02-20T13:16:47.230235Z","shell.execute_reply":"2022-02-20T13:16:47.235858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install albumentations","metadata":{"execution":{"iopub.status.busy":"2022-02-20T13:16:50.192816Z","iopub.execute_input":"2022-02-20T13:16:50.193807Z","iopub.status.idle":"2022-02-20T13:16:59.442855Z","shell.execute_reply.started":"2022-02-20T13:16:50.193754Z","shell.execute_reply":"2022-02-20T13:16:59.441743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow\nimport albumentations as albu\n\nclass DataGenerator(tensorflow.keras.utils.Sequence):\n    def __init__(self, images, labels=None, mode='fit', batch_size=batch_size, dim=(height, width), channels=channels, n_classes=n_classes, shuffle=True, augment=False):\n        \n        #initializing the configuration of the generator\n        self.images = images\n        self.labels = labels\n        self.mode = mode\n        self.batch_size = batch_size\n        self.dim = dim\n        self.channels = channels\n        self.n_classes = n_classes\n        self.shuffle = shuffle\n        self.augment = augment\n        self.on_epoch_end()\n   \n    #method to be called after every epoch\n    def on_epoch_end(self):\n        self.indexes = np.arange(self.images.shape[0])\n        if self.shuffle == True:\n            np.random.shuffle(self.indexes)\n    \n    #return numbers of steps in an epoch using samples and batch size\n    def __len__(self):\n        return int(np.floor(len(self.images) / self.batch_size))\n    \n    #this method is called with the batch number as an argument to obtain a given batch of data\n    def __getitem__(self, index):\n        #generate one batch of data\n        #generate indexes of batch\n        batch_indexes = self.indexes[index * self.batch_size:(index+1) * self.batch_size]\n        \n        #generate mini-batch of X\n        X = np.empty((self.batch_size, *self.dim, self.channels))\n        \n        for i, ID in enumerate(batch_indexes):\n            #generate pre-processed image\n            img = self.images[ID]\n            #image rescaling\n            img = img.astype(np.float32)/255.\n            #resizing as per new dimensions\n#             img = resize_img(img, self.dim)\n            X[i] = img\n            \n        #generate mini-batch of y\n        if self.mode == 'fit':\n            y = self.labels[batch_indexes]\n            \n            #augmentation on the training dataset\n            if self.augment == True:\n                X = self.__augment_batch(X)\n            return X, y\n        \n        elif self.mode == 'predict':\n            return X\n        \n        else:\n            raise AttributeError(\"The mode should be set to either 'fit' or 'predict'.\")\n            \n    #augmentation for one image\n    def __random_transform(self, img):\n        composition = albu.Compose([albu.HorizontalFlip(p=0.5),\n                                   albu.VerticalFlip(p=0.5),\n                                   albu.GridDistortion(p=0.2),\n                                   albu.ElasticTransform(p=0.2)])\n        return composition(image=img)['image']\n    \n    #augmentation for batch of images\n    def __augment_batch(self, img_batch):\n        for i in range(img_batch.shape[0]):\n            img_batch[i] = self.__random_transform(img_batch[i])\n        return img_batch\n    \nprint(\"completed\")","metadata":{"execution":{"iopub.status.busy":"2022-02-20T13:17:05.790613Z","iopub.execute_input":"2022-02-20T13:17:05.790917Z","iopub.status.idle":"2022-02-20T13:17:06.825846Z","shell.execute_reply.started":"2022-02-20T13:17:05.790885Z","shell.execute_reply":"2022-02-20T13:17:06.824665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_data_generator = DataGenerator(X_train_data, y_train_data, augment=True)\ntrain_data_generator = DataGenerator(X_train_data, y_train_data, augment=False)\nvalid_data_generator = DataGenerator(X_val_data, y_val_data, augment=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-20T13:17:27.613048Z","iopub.execute_input":"2022-02-20T13:17:27.6136Z","iopub.status.idle":"2022-02-20T13:17:27.62006Z","shell.execute_reply.started":"2022-02-20T13:17:27.613562Z","shell.execute_reply":"2022-02-20T13:17:27.61884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train Model","metadata":{}},{"cell_type":"markdown","source":"#### build model","metadata":{}},{"cell_type":"code","source":"# res_model = ResNet50(include_top=False,\n#                     weights=\"imagenet\",\n#                     input_shape=input_shape)\n\n\nres_model = tf.keras.applications.ResNet101(include_top=False,\n                                            weights=\"imagenet\",\n                                            input_shape=input_shape)","metadata":{"execution":{"iopub.status.busy":"2022-02-20T13:17:34.209735Z","iopub.execute_input":"2022-02-20T13:17:34.210598Z","iopub.status.idle":"2022-02-20T13:17:41.826218Z","shell.execute_reply.started":"2022-02-20T13:17:34.210562Z","shell.execute_reply":"2022-02-20T13:17:41.825226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_model = res_model\nx = base_model\n\nfor layer in base_model.layers:\n    layer.trainable = False\n    \n# un-freeze the BatchNorm layers\nfor layer in base_model.layers:\n    if \"BatchNormalization\" in layer.__class__.__name__:\n        layer.trainable = True","metadata":{"execution":{"iopub.status.busy":"2022-02-20T13:17:46.726941Z","iopub.execute_input":"2022-02-20T13:17:46.727277Z","iopub.status.idle":"2022-02-20T13:17:46.756595Z","shell.execute_reply.started":"2022-02-20T13:17:46.727232Z","shell.execute_reply":"2022-02-20T13:17:46.755653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# x = res_model.output\n# x = GlobalAveragePooling2D()(x)\n# x = Dropout(0.5)(x)\n# predictions = Dense(n_classes, activation= 'softmax')(x)\n# model = Model(inputs = res_model.input, outputs = predictions)\n\nimport tensorflow.keras as K\nmodel = K.models.Sequential()\nmodel.add(x)\nmodel.add(K.layers.Flatten())\nmodel.add(K.layers.BatchNormalization())\nmodel.add(K.layers.Dense(256, activation='relu'))\nmodel.add(K.layers.Dropout(0.5))\nmodel.add(K.layers.BatchNormalization())\nmodel.add(K.layers.Dense(128, activation='relu'))\nmodel.add(K.layers.Dropout(0.5))\nmodel.add(K.layers.BatchNormalization())\nmodel.add(K.layers.Dense(64, activation='relu'))\nmodel.add(K.layers.Dropout(0.5))\nmodel.add(K.layers.BatchNormalization())\nmodel.add(K.layers.Dense(n_classes, activation='softmax'))\n\nprint(\"added\")","metadata":{"execution":{"iopub.status.busy":"2022-02-20T13:17:50.259098Z","iopub.execute_input":"2022-02-20T13:17:50.259698Z","iopub.status.idle":"2022-02-20T13:17:51.327489Z","shell.execute_reply.started":"2022-02-20T13:17:50.259661Z","shell.execute_reply":"2022-02-20T13:17:51.326524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# keras.optimizers.Adam(learning_rate=0.0001)\n# K.optimizers.RMSprop(learning_rate=2e-5),\nmodel.compile(loss='categorical_crossentropy',\n                  optimizer=K.optimizers.Adam(learning_rate=0.0001),\n                  metrics=['categorical_accuracy'])","metadata":{"execution":{"iopub.status.busy":"2022-02-20T13:17:55.624849Z","iopub.execute_input":"2022-02-20T13:17:55.625152Z","iopub.status.idle":"2022-02-20T13:17:55.652146Z","shell.execute_reply.started":"2022-02-20T13:17:55.625119Z","shell.execute_reply":"2022-02-20T13:17:55.651223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### prepare callbacks","metadata":{}},{"cell_type":"code","source":"# we want only the very best version of the model and we define ‘best’ as the one with the lowest validation loss.\n# best_model = \"../input/bestmodelhdf5/best_model.hdf5\"\nbest_model = \"./kaggle/working/best_model.hdf5\"\ncheckpoint = ModelCheckpoint(\n    filepath=best_model,\n    monitor='val_loss',\n    verbose=1,\n    save_best_only=True,\n    mode='min')\n\n#early stopping to monitor the validation loss and avoid overfitting\nearly_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10, restore_best_weights=True)\n\n#reducing learning rate on plateau\nrlrop = ReduceLROnPlateau(monitor='val_loss', mode='min', patience= 5, factor= 0.5, min_lr= 1e-6, verbose=1)\n\nprint(\"completed\")","metadata":{"execution":{"iopub.status.busy":"2022-02-20T13:17:58.478925Z","iopub.execute_input":"2022-02-20T13:17:58.479252Z","iopub.status.idle":"2022-02-20T13:17:58.487466Z","shell.execute_reply.started":"2022-02-20T13:17:58.479219Z","shell.execute_reply":"2022-02-20T13:17:58.486334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### use saved model","metadata":{}},{"cell_type":"code","source":"model = load_model(best_model)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### fit model","metadata":{}},{"cell_type":"code","source":"# history = model.fit(X_train, Y_train, \n#                     batch_size=32, epochs=80,\n#                     callbacks=[checkpoint, rlrop],\n#                     verbose=1, validation_split=0.2, shuffle=True)\n\nhistory = model.fit(train_data_generator,\n                    validation_data=valid_data_generator,\n                    # callbacks=[checkpoint, early_stop, rlrop],\n                    callbacks=[checkpoint, rlrop],\n                    verbose=1,\n                    epochs=80,\n                    use_multiprocessing=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-20T13:18:09.837286Z","iopub.execute_input":"2022-02-20T13:18:09.837583Z","iopub.status.idle":"2022-02-20T14:07:57.114953Z","shell.execute_reply.started":"2022-02-20T13:18:09.83755Z","shell.execute_reply":"2022-02-20T14:07:57.113613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(history.history.keys())","metadata":{"execution":{"iopub.status.busy":"2022-02-20T12:11:28.950539Z","iopub.execute_input":"2022-02-20T12:11:28.951191Z","iopub.status.idle":"2022-02-20T12:11:28.95682Z","shell.execute_reply.started":"2022-02-20T12:11:28.951152Z","shell.execute_reply":"2022-02-20T12:11:28.956066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plot to visualize the loss and accuracy against number of epochs\n# history\nmodel_history = history\nplt.figure(figsize=(18,8))\n\nplt.suptitle('Loss and Accuracy Plots', fontsize=18)\n\nplt.subplot(1,2,1)\nplt.plot(model_history.history['loss'], label='Training Loss')\nplt.plot(model_history.history['val_loss'], label='Validation Loss')\nplt.legend()\nplt.xlabel('Number of epochs', fontsize=15)\nplt.ylabel('Loss', fontsize=15)\n\nplt.subplot(1,2,2)\nplt.plot(model_history.history['categorical_accuracy'], label='Train Accuracy')\nplt.plot(model_history.history['val_categorical_accuracy'], label='Validation Accuracy')\nplt.legend()\nplt.xlabel('Number of epochs', fontsize=14)\nplt.ylabel('Accuracy', fontsize=14)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-20T14:10:05.058886Z","iopub.execute_input":"2022-02-20T14:10:05.059239Z","iopub.status.idle":"2022-02-20T14:10:05.59904Z","shell.execute_reply.started":"2022-02-20T14:10:05.059203Z","shell.execute_reply":"2022-02-20T14:10:05.598059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### finding test accuracy","metadata":{}},{"cell_type":"code","source":"print(Y_test_orig.shape)","metadata":{"execution":{"iopub.status.busy":"2022-02-20T12:12:29.632478Z","iopub.execute_input":"2022-02-20T12:12:29.632744Z","iopub.status.idle":"2022-02-20T12:12:29.639275Z","shell.execute_reply.started":"2022-02-20T12:12:29.632702Z","shell.execute_reply":"2022-02-20T12:12:29.638416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict_labels = model.predict(X_test)\n# reversing one-hot\nimport numpy as np\ny_pred = np.argmax(predict_labels, axis=1) # assuming you have n-by-5 class_prob\n# print(y_pred.shape)\n\nfrom sklearn.metrics import accuracy_score\ntest_accuracy = accuracy_score(Y_test_orig, y_pred)\nprint('Test Accuracy: ', round((test_accuracy * 100), 2), \"%\")\n\nfrom sklearn.metrics import f1_score\nmacro_f1 = f1_score(Y_test_orig, y_pred, average=\"macro\")\nprint( 'macro-f1: ' + str(macro_f1))\n\nfrom sklearn.metrics import classification_report\nlabels = ['1', '2', '3', '4']\nprint(classification_report(Y_test_orig, y_pred, target_names=labels))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_factor = int(X_test_data.shape[0] / batch_size)\ntest_limit = batch_size * test_factor\n\nX_test_data = X_test_data[:test_limit]\ny_test_data = Y_test_orig[:test_limit]\nprint(y_test_data.shape)\n\n\ny_pred = model.predict(DataGenerator(X_test_data, mode='predict', augment=False, shuffle=False), verbose=1)\ny_pred = np.argmax(y_pred, axis=1)\n\nprint(y_pred.shape)","metadata":{"execution":{"iopub.status.busy":"2022-02-20T14:13:25.928081Z","iopub.execute_input":"2022-02-20T14:13:25.928949Z","iopub.status.idle":"2022-02-20T14:13:31.267881Z","shell.execute_reply.started":"2022-02-20T14:13:25.92891Z","shell.execute_reply":"2022-02-20T14:13:31.266846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import itertools\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.figure(figsize = (5,5))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=90)\n    plt.yticks(tick_marks, classes)\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","metadata":{"execution":{"iopub.status.busy":"2022-02-20T14:17:28.469933Z","iopub.execute_input":"2022-02-20T14:17:28.470288Z","iopub.status.idle":"2022-02-20T14:17:28.480621Z","shell.execute_reply.started":"2022-02-20T14:17:28.470229Z","shell.execute_reply":"2022-02-20T14:17:28.479529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_accuracy = accuracy_score(y_test_data, y_pred)\n\nprint('Test Accuracy: ', round((test_accuracy * 100), 2), \"%\")\n\nfrom sklearn.metrics import f1_score\nmacro_f1 = f1_score(y_test_data, y_pred, average=\"macro\")\nprint( 'macro-f1: ' + str(macro_f1))\n\ndict_characters = {1:'NEUTROPHIL',2:'EOSINOPHIL',3:'MONOCYTE',4:'LYMPHOCYTE'}\nfrom sklearn.metrics import classification_report\nlabels = list(dict_characters.values())\nprint(classification_report(y_test_data, y_pred, target_names=labels))\n\nfrom sklearn.metrics import confusion_matrix\nconfusion_mtx = confusion_matrix(y_test_data, y_pred) \nplot_confusion_matrix(confusion_mtx, classes = list(dict_characters.values())) \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-20T14:17:29.826126Z","iopub.execute_input":"2022-02-20T14:17:29.826801Z","iopub.status.idle":"2022-02-20T14:17:30.164841Z","shell.execute_reply.started":"2022-02-20T14:17:29.826735Z","shell.execute_reply":"2022-02-20T14:17:30.163927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}